{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convolutional Layer Network\n",
    "\n",
    "Convolutional Neural Networks, or CNNs, are a class of deep neural networks particularly suited for image recognition and classification tasks. They have revolutionized the field of computer vision by achieving remarkable performance in tasks such as object detection, image segmentation, and facial recognition. In this lecture, we will explore the fundamental concepts behind CNNs and understand how they work.\n",
    "\n",
    "\n",
    "* Convolutional Layers:\n",
    "\n",
    "The core building blocks of CNNs are convolutional layers, which apply convolution operations to input images.\n",
    "Each convolutional layer consists of multiple filters (also known as kernels) that slide over the input image, performing element-wise multiplication and summation to produce feature maps.\n",
    "These filters capture different patterns and features such as edges, textures, and shapes.\n",
    "\n",
    "* Pooling Layers:\n",
    "\n",
    "Pooling layers are used to downsample feature maps, reducing computational complexity and controlling overfitting.\n",
    "Common pooling operations include max pooling and average pooling, which respectively retain the maximum and average values within defined regions.\n",
    "\n",
    "* Activation Functions:\n",
    "\n",
    "Activation functions introduce non-linearity into the network, enabling it to learn complex mappings between inputs and outputs.\n",
    "Popular activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
    "\n",
    "* Fully Connected Layers:\n",
    "\n",
    "Fully connected layers connect every neuron in one layer to every neuron in the next layer, allowing the network to learn high-level representations.\n",
    "These layers are typically used in the final stages of the network for classification or regression tasks.\n",
    "\n",
    "* Training Process:\n",
    "1. Forward Propagation:\n",
    "\n",
    "        During forward propagation, input images are passed through the network, and computations are performed layer by layer to generate predictions.\n",
    "2. Loss Calculation:\n",
    "\n",
    "        The difference between the predicted output and the ground truth labels is quantified using a loss function, such as cross-entropy loss for classification tasks.\n",
    "3. Backward Propagation (Backpropagation):\n",
    "\n",
    "        Backpropagation is used to compute the gradients of the loss function with respect to the network parameters. These gradients are then used to update the parameters using optimization algorithms like stochastic gradient descent (SGD) or Adam.\n",
    "4. Iterative Optimization:\n",
    "\n",
    "        The training process iterates through multiple epochs, with each epoch consisting of forward and backward propagation steps. Over time, the network learns to minimize the loss function and improve its performance on the training data.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Images/image-13.png\" alt=\"Alt text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Semantic Segmentation:\n",
    "\n",
    "Semantic segmentation extends the capabilities of CNNs by providing pixel-level understanding of images, enabling the partitioning of an image into semantically meaningful regions and assigning class labels to individual pixels.\n",
    "\n",
    "* Pixel-Level Classification\n",
    "        \n",
    "        Unlike image classification tasks that assign a single label to the entire image, semantic segmentation assigns class labels to each pixel within the image, resulting in a detailed semantic understanding of the scene.\n",
    "\n",
    "* Encoder-Decoder Architectures\n",
    "        \n",
    "        Many semantic segmentation models employ encoder-decoder architectures, where the encoder extracts hierarchical features from the input image, and the decoder generates pixel-wise predictions by upsampling the feature maps to the original resolution.\n",
    "\n",
    "* Loss Functions\n",
    "        \n",
    "        Semantic segmentation models are trained using loss functions that measure the discrepancy between predicted pixel-wise labels and ground truth annotations. Common loss functions include cross-entropy loss, dice loss, and intersection-over-union (IoU) loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Model Implementation\n",
    "\n",
    "For the implementation of our semantic segmentation model, it is advisable to employ the following architecture: (Of course, you can create your own architecture)\n",
    "\n",
    "* TASK: select the suitbale optimizer, loss function and respective metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "################################################################\n",
    "def simple_unet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
    "#Build the model\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
    "    s = inputs\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "     \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "     \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "     \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    \n",
    "    #Expansive path \n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "     \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "     \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "     \n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "     \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "     \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1. Data Preparation for The Training Step\n",
    "\n",
    "Use the images that you previously labeled.\n",
    "* TASK:\n",
    "    * Implement a Python script to:\n",
    "        * Read each image and mask using OpenCV.\n",
    "        * Convert them to grayscale.\n",
    "        * Resize to SIZE x SIZE.\n",
    "        * Convert to numpy arrays.\n",
    "        * Append to image_dataset and mask_dataset list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 512\n",
    "image_dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.  \n",
    "mask_dataset = []  #Place holders to define add labels. We will add 0 to all parasitized images and 1 to uninfected.\n",
    "validation_dataset = []\n",
    "\n",
    "images = os.listdir(image_directory)\n",
    "for i, image_name in enumerate(images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        #print(image_directory+image_name)\n",
    "        image = cv2.imread(image_directory+image_name, 0)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        image_dataset.append(np.array(image))\n",
    "\n",
    "#Iterate through all images in Uninfected folder, resize to 64 x 64\n",
    "#Then save into the same numpy array 'dataset' but with label 1\n",
    "\n",
    "masks = os.listdir(mask_directory)\n",
    "for i, image_name in enumerate(masks):\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        image = cv2.imread(mask_directory+image_name, 0)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        mask_dataset.append(np.array(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.2. Data Preparation for The Training Step (Normalize image pixels and rescale mask pixels for semantic segmentation)\n",
    "\n",
    "* TASK:\n",
    "    * Normalize pixel values of images in image_dataset using the normalize function along the channel axis.\n",
    "    * Ensure pixel values of images are normalized across all channels.\n",
    "    * Rescale pixel values of masks in mask_dataset to [0, 1] by dividing each pixel value by 255.\n",
    "    * Add an extra dimension to both datasets to match the model's input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize images\n",
    "image_dataset = np.expand_dims(normalize(np.array(image_dataset), axis=1),3)\n",
    "#D not normalize masks, just rescale to 0 to 1.\n",
    "mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.3. Data Preparation for The Training Step (Split image and mask data with train_test_split)\n",
    "\n",
    "* TASK:\n",
    "    * Import the train_test_split function from sklearn.model_selection.\n",
    "    * Split image_dataset and mask_dataset into training and testing sets:\n",
    "    * Assign 90% of the data to the training set (X_train, y_train).\n",
    "    * Assign 10% of the data to the testing set (X_test, y_test).\n",
    "    * Use a test_size of 0.10 to specify the proportion of data for testing.\n",
    "    * Set random_state to 0 for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.10, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Train The Model\n",
    "\n",
    "* TASK:\n",
    "    * Define the dimensions of the image dataset (image_dataset) as follows:\n",
    "        * IMG_HEIGHT = image_dataset.shape[1]\n",
    "        * IMG_WIDTH = image_dataset.shape[2]\n",
    "        * IMG_CHANNELS = image_dataset.shape[3]\n",
    "    * Implement a function to retrieve a U-Net model with the specified dimensions\n",
    "    * Train the model using the training data (X_train, y_train) and validate it using the testing data (X_test, y_test)\n",
    "    * Save the trained model to the specified location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check, view few mages\n",
    "import random\n",
    "import numpy as np\n",
    "image_number = random.randint(0, len(X_train))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.reshape(X_train[image_number], (512, 512)), cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.reshape(y_train[image_number], (512, 512)), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = image_dataset.shape[1]\n",
    "IMG_WIDTH  = image_dataset.shape[2]\n",
    "IMG_CHANNELS = image_dataset.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return simple_unet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If starting with pre-trained weights. \n",
    "#model.load_weights('mitochondria_gpu_tf1.4.hdf5')\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size = 16, \n",
    "                    verbose=1, \n",
    "                    epochs=5, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    shuffle=False)\n",
    "\n",
    "model.save(r'D:\\02-Project\\01-Wear Detection\\07-Scripts\\debug_environment\\debug_environment\\Model\\TEST_Ehsan_Aug_Test.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. Extracting Accuracy Parameters\n",
    "Now, let's delve into the process of extracting accuracy parameters from our model. This task involves two main components:\n",
    "\n",
    "TASK:\n",
    "\n",
    "* Extracting Accuracy and Loss Diagrams with Epochs:\n",
    "    * One fundamental aspect of assessing a model's performance is tracking its accuracy and loss over epochs. By visualizing these metrics over the course of training, we gain insights into how well our model is learning and whether it's overfitting or underfitting. We'll extract these diagrams to analyze the trends and make informed decisions about model adjustments.\n",
    "\n",
    "* Calculating the Intersection over Union (IOU) Level:\n",
    "    * Another critical metric for evaluating the performance of models, especially in tasks like object detection and semantic segmentation, is the Intersection over Union (IOU). This metric quantifies the overlap between predicted and ground-truth bounding boxes or segmentation masks. By calculating the IOU level, we can gauge how accurately our model is delineating objects or regions of interest within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "\n",
    "\n",
    "\t# evaluate model\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "#acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "#val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IOU\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_thresholded = y_pred > 0.5\n",
    "\n",
    "intersection = np.logical_and(y_test, y_pred_thresholded)\n",
    "union = np.logical_or(y_test, y_pred_thresholded)\n",
    "iou_score = np.sum(intersection) / np.sum(union)\n",
    "print(\"IoU socre is: \", iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
